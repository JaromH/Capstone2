{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22cde886",
   "metadata": {},
   "source": [
    "# Capstone 2: Pre-processing and Training Data Development<a id='Pre-processing_and_Training_Data_Development'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b9dcd",
   "metadata": {},
   "source": [
    "## Contents<a id='Contents'></a>\n",
    "* [Pre-processing and Training Data Development](#Pre-processing_and_Training_Data_Development)\n",
    "  * [Contents](#Contents)\n",
    "  * [Introduction](#Introduction)\n",
    "  * [Imports](#Imports)\n",
    "  * [Load the Data](#Load_the_Data)\n",
    "  * [Training and Test Sets](#Training_and_Test_Sets)\n",
    "    * [Training and test split with markdown columns](#Split_with_markdown_columns)\n",
    "    * [Training and test split without markdown columns](#Split_without_markdown_columns)\n",
    "  * [Scale the Data](#Scale_the_Data)\n",
    "  * [Export the Data](#Export_the_Data)\n",
    "  * [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f91a27",
   "metadata": {},
   "source": [
    "## Introduction<a id='Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865e792",
   "metadata": {},
   "source": [
    "After running through the expoloritory data analysis, I've found that we need to take a closer look at some of our features to truly see if I can predict the weekly sales and which features will most acurately predict this. In order to prepare my data for machine learning models, I will need to create training and testing datasets as well as apply any scaling need for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b810ca3",
   "metadata": {},
   "source": [
    "## Imports<a id='Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c499a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "# Import relevant libraries and packages.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # For all our visualization needs.\n",
    "import statsmodels.api as sm #This is a python module which provides classes and functions for the estimation of different statistical models, conducting statistical tests, and statistical EDA.\n",
    "from statsmodels.graphics.api import abline_plot # For visualizing evaluating predictions.\n",
    "from sklearn.metrics import mean_squared_error, r2_score #The mean_squared error is the average squared difference between the estimated values and true value. The r2_score is used to determine how the variability of one factor can be caused by its relationship to another related factor.\n",
    "from sklearn.model_selection import train_test_split # To split the data.\n",
    "from sklearn import linear_model, preprocessing # The linear model is the ordinary least squares linear regression model. Preprocessing helps to standardize a data set. If some outliers are present in the set, robust scalers or transformers are more appropriate.\n",
    "import warnings # For handling error messages.\n",
    "# Don't worry about the following two instructions: they just suppress warnings that could occur later. \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', module='scipy', message='^internal gelsd')\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9185d",
   "metadata": {},
   "source": [
    "## Load the Data<a id='Load_the_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c810a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CSV data from the data wrangling\n",
    "data_all_0 = pd.read_csv('C:/Users/jmhat/Desktop/Coding/Capstone2/data/data_all_0.csv')\n",
    "data_no_nan = pd.read_csv('C:/Users/jmhat/Desktop/Coding/Capstone2/data/data_no_nan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe76c60",
   "metadata": {},
   "source": [
    "## Split data into training and test sets<a id='Training_and_Test_Sets'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fc5f7",
   "metadata": {},
   "source": [
    "Using the train test split function, I'll split both the data with and without the markdown columns. I've used a split of 80/20 training to test samples and a random state of 42 for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55adcc1",
   "metadata": {},
   "source": [
    "### Training and test split with markdown columns<a id='Split_with_markdown_columns'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff0e01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sections\n",
    "X_0 = data_all_0.drop(columns=['Weekly_Sales'])\n",
    "y_0 = data_all_0['Weekly_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec379ad",
   "metadata": {},
   "source": [
    "### Training and test split without markdown columns<a id='Split_without_markdown_columns'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d13abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nan = data_no_nan.drop(columns=['Weekly_Sales'])\n",
    "y_nan = data_no_nan['Weekly_Sales']\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_nan, y_nan, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88b50d",
   "metadata": {},
   "source": [
    "## Scale the training and testing data<a id='Scale_the_Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbda2a3",
   "metadata": {},
   "source": [
    "There are a few common scalers, but I've decided to use the standard scaler and min max scaler methods for the data. I will also export the unscaled data for use in tree based machine learning models. If I were to go on to create a neural network I would also need to make sure I didn't have any negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f70ee50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_scaler = StandardScaler()\n",
    "MM_scaler = MinMaxScaler()\n",
    "\n",
    "SS_scaler.fit(X_train)\n",
    "MM_scaler.fit(X_train)\n",
    "\n",
    "XtrainSS = SS_scaler.transform(X_train)\n",
    "XtrainMM = MM_scaler.transform(X_train)\n",
    "\n",
    "#XSS_train = pd.DataFrame(XtrainSS, columns=X_0.columns)\n",
    "#XMM_train = pd.DataFrame(XtrainMM, columns=X_0.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f824f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_scaler.fit(X1_train)\n",
    "MM_scaler.fit(X1_train)\n",
    "\n",
    "X1trainSS = SS_scaler.transform(X1_train)\n",
    "X1trainMM = MM_scaler.transform(X1_train)\n",
    "\n",
    "#X1SS_train = pd.DataFrame(X1trainSS, columns=X_nan.columns)\n",
    "#X1MM_train = pd.DataFrame(X1trainMM, columns=X_nan.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c448bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_scaler.fit(X_test)\n",
    "MM_scaler.fit(X_test)\n",
    "\n",
    "XtestSS = SS_scaler.transform(X_test)\n",
    "XtestMM = MM_scaler.transform(X_test)\n",
    "\n",
    "#XSS_test = pd.DataFrame(XtestSS, columns=X_0.columns)\n",
    "#XMM_test = pd.DataFrame(XtestMM, columns=X_0.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef609df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_scaler.fit(X1_test)\n",
    "MM_scaler.fit(X1_test)\n",
    "\n",
    "X1testSS = SS_scaler.transform(X1_test)\n",
    "X1testMM = MM_scaler.transform(X1_test)\n",
    "\n",
    "#X1SS_test = pd.DataFrame(X1testSS, columns=X_nan.columns)\n",
    "#X1MM_test = pd.DataFrame(X1testMM, columns=X_nan.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b28e8",
   "metadata": {},
   "source": [
    "## Export Test and Training Data<a id='Export_the_Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba864fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a new csv file\n",
    "#XtrainSS.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/XtrainSS.csv\", index=False)\n",
    "np.savetxt('XtrainSS.csv', XtrainSS, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "759cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XtrainMM.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/XtrainMM.csv\", index=False)\n",
    "np.savetxt('XtrainMM.csv', XtrainMM, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2afa48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1trainSS.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1trainSS.csv\", index=False)\n",
    "np.savetxt('X1trainSS.csv', X1trainSS, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db871447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1trainMM.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1trainMM.csv\", index=False)\n",
    "np.savetxt('X1trainMM.csv', X1trainMM, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "023efdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XtestSS.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/XtestSS.csv\", index=False)\n",
    "np.savetxt('XtestSS.csv', XtestSS, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac624393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XtestMM.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/XtestMM.csv\", index=False)\n",
    "np.savetxt('XtestMM.csv', XtestMM, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb95579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1testSS.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1testSS.csv\", index=False)\n",
    "np.savetxt('X1testSS.csv', X1testSS, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60c56964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1testMM.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1testMM.csv\", index=False)\n",
    "np.savetxt('X1testMM.csv', X1testMM, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "840dc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X_train.csv\", index=False)\n",
    "np.savetxt('X_train.csv', X_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "231607bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X_test.csv\", index=False)\n",
    "np.savetxt('X_test.csv', X_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2c48b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/y_train.csv\", index=False)\n",
    "np.savetxt('y_train.csv', y_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acfe37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/y_test.csv\", index=False)\n",
    "np.savetxt('y_test.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7a73f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1_train.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1_train.csv\", index=False)\n",
    "np.savetxt('X1_train.csv', X1_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24e34d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1_test.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/X1_test.csv\", index=False)\n",
    "np.savetxt('X1_test.csv', X1_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e054b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1_train.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/y1_train.csv\", index=False)\n",
    "np.savetxt('y1_train.csv', y1_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2ad8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1_test.to_csv(\"C:/Users/jmhat/Desktop/Coding/Capstone2/data/y1_test.csv\", index=False)\n",
    "np.savetxt('y1_test.csv', y1_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf3d1d",
   "metadata": {},
   "source": [
    "## Summary<a id='Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1108d699",
   "metadata": {},
   "source": [
    "In this notebook, I finished the final pre-processing and split the data from the two dataframes into training and test elements. I will now be able to use these sets of data to run machine learning models to determine if there are any features that accuractely predict the weekly sales experienced by these stores. I will also use the scaled and unscaled data to determine if scaling helps my machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
